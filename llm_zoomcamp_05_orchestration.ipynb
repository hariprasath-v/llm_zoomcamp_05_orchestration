{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About\n",
    "#### Exploring mage hybrid framework\n",
    "#### Mage is an open-source hybrid framework for ETL.\n",
    "#### The following tasks are explored using the mage framework\n",
    "#### * Data ingestion\n",
    "#### * Data chunking\n",
    "#### * Data export using elasticsearch\n",
    "#### * Data retrieval using elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Github codespaces utilized for this task\n",
    "#### Steps to run the mage.ai\n",
    "```\n",
    "Clone repository\n",
    "git clone https://github.com/mage-ai/rag-project\n",
    "cd rag-project\n",
    "navigate to the rag-project/llm directory, add spacy to the requirements.txt.\n",
    "Then update the Dockerfile found in the rag-project directory with the following:\n",
    "RUN python -m spacy download en_core_web_sm\n",
    "Run\n",
    "./scripts/start.sh\n",
    "Once started, go to http://localhost:6789/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Running Mage\n",
    "#### What's the version of mage?\n",
    "#### Answer: v0.9.72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For FAQ document: version 1\n",
    "#### 1. Select New --> and create Retrieval Augmented Generation pipeline\n",
    "#### 2. Select Data preparation and Go\n",
    "#### 3. Select Load and Go\n",
    "#### 4. In the load select Ingest and Go\n",
    "#### 5. In the ingest select Add block --> custom code\n",
    "#### Paste the following code in edit section\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_ingest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_ingest.py\n",
    "#Data ingest\n",
    "import io\n",
    "\n",
    "import requests\n",
    "import docx\n",
    "\n",
    "\n",
    "\n",
    "if 'data_loader' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import data_loader\n",
    "if 'test' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import test\n",
    "\n",
    "\n",
    "@data_loader\n",
    "def load_data(*args, **kwargs):\n",
    "    def clean_line(line):\n",
    "        line = line.strip()\n",
    "        line = line.strip('\\uFEFF')\n",
    "        return line\n",
    "    \n",
    "    def read_faq(file_id):\n",
    "        url = f'https://docs.google.com/document/d/{file_id}/export?format=docx'\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with io.BytesIO(response.content) as f_in:\n",
    "            doc = docx.Document(f_in)\n",
    "\n",
    "        questions = []\n",
    "\n",
    "        question_heading_style = 'heading 2'\n",
    "        section_heading_style = 'heading 1'\n",
    "        \n",
    "        heading_id = ''\n",
    "        section_title = ''\n",
    "        question_title = ''\n",
    "        answer_text_so_far = ''\n",
    "        \n",
    "        for p in doc.paragraphs:\n",
    "            style = p.style.name.lower()\n",
    "            p_text = clean_line(p.text)\n",
    "        \n",
    "            if len(p_text) == 0:\n",
    "                continue\n",
    "        \n",
    "            if style == section_heading_style:\n",
    "                section_title = p_text\n",
    "                continue\n",
    "        \n",
    "            if style == question_heading_style:\n",
    "                answer_text_so_far = answer_text_so_far.strip()\n",
    "                if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "                    questions.append({\n",
    "                        'text': answer_text_so_far,\n",
    "                        'section': section_title,\n",
    "                        'question': question_title,\n",
    "                    })\n",
    "                    answer_text_so_far = ''\n",
    "        \n",
    "                question_title = p_text\n",
    "                continue\n",
    "            \n",
    "            answer_text_so_far += '\\n' + p_text\n",
    "        \n",
    "        answer_text_so_far = answer_text_so_far.strip()\n",
    "        if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "            questions.append({\n",
    "                'text': answer_text_so_far,\n",
    "                'section': section_title,\n",
    "                'question': question_title,\n",
    "            })\n",
    "\n",
    "        return questions\n",
    "\n",
    "    faq_documents = {\n",
    "                'llm-zoomcamp': '1qZjwHkvP0lXHiE4zdbWyUXSVfmVGzougDD6N37bat3E',\n",
    "            }\n",
    "    documents = []\n",
    "\n",
    "    for course, file_id in faq_documents.items():\n",
    "    \n",
    "        course_documents = read_faq(file_id)\n",
    "        documents.append({'course': course, 'documents': course_documents})\n",
    "    print('Length:',len(documents))\n",
    "    return [documents]\n",
    "\n",
    "@test\n",
    "def test_output(output, *args) -> None:\n",
    "    \"\"\"\n",
    "    Template code for testing the output of the block.\n",
    "    \"\"\"\n",
    "    assert output is not None, 'The output is undefined'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Reading the documents\n",
    "```\n",
    "Now we can ingest the documents. Create a custom code ingestion block\n",
    "\n",
    "Let's read the documents. We will use the same code we used for parsing FAQ: parse-faq-llm.ipynb\n",
    "\n",
    "Use the following document_id: 1qZjwHkvP0lXHiE4zdbWyUXSVfmVGzougDD6N37bat3E\n",
    "\n",
    "Which is the document ID of LLM FAQ version 1\n",
    "\n",
    "```\n",
    "#### How many FAQ documents we processed?\n",
    "#### Answer: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "#### 1. Select data preparation --> transform --> chunking\n",
    "#### 2. In chunking select Add block --> custom code\n",
    "#### Paste the following code in edit section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_chunking.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_chunking.py\n",
    "#Data chunking\n",
    "import re\n",
    "from typing import Any, Dict, List\n",
    "import hashlib\n",
    "\n",
    "\n",
    "@transformer\n",
    "def chunk_documents(data: List[Dict[str, Any]], *args, **kwargs):\n",
    "    # Generate a unique document ID\n",
    "    def generate_document_id(doc):\n",
    "        combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n",
    "        hash_object = hashlib.md5(combined.encode())\n",
    "        hash_hex = hash_object.hexdigest()\n",
    "        document_id = hash_hex[:8]\n",
    "        return document_id\n",
    "    documents = []\n",
    "    for course_dict in data:\n",
    "        for doc in course_dict['documents']:\n",
    "            doc['course'] = course_dict['course']\n",
    "            #print(doc)\n",
    "            #break\n",
    "            # previously we used just \"id\" for document ID\n",
    "            doc['document_id'] = generate_document_id(doc)\n",
    "            documents.append(doc)\n",
    "    print(f'Documents:', len(documents))\n",
    "    return [documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Chunking\n",
    "#### How many documents (chunks) do we have in the output?\n",
    "#### Answer: \n",
    "```\n",
    "Documents: 86\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export\n",
    "#### 1. Select data preparation --> export --> vector database\n",
    "#### 2. In vector database select Add block --> custom code\n",
    "#### Paste the following code in edit section\n",
    "#### Change the connection string as http://elasticsearch:9200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_export_elasticsearch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_export_elasticsearch.py\n",
    "#Data export\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "from datetime import datetime\n",
    "from mage_ai.data_preparation.variable_manager import set_global_variable\n",
    "\n",
    "@data_exporter\n",
    "def elasticsearch(documents, *args, **kwargs):\n",
    "    connection_string = kwargs.get('connection_string', 'http://localhost:9200')\n",
    "    index_name_prefix = kwargs.get('index_name', 'documents')\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%M%S\")\n",
    "    index_name = f\"{index_name_prefix}_{current_time}\"\n",
    "    print(\"index name:\", index_name)\n",
    "    set_global_variable('ominous_maelstrom', 'index_name', index_name)\n",
    "    number_of_shards = kwargs.get('number_of_shards', 1)\n",
    "    number_of_replicas = kwargs.get('number_of_replicas', 0)\n",
    "    dimensions = kwargs.get('dimensions')\n",
    "\n",
    "    \n",
    "\n",
    "    es_client = Elasticsearch(connection_string)\n",
    "\n",
    "    print(f'Connecting to Elasticsearch at {connection_string}')\n",
    "\n",
    "    index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": number_of_shards,\n",
    "        \"number_of_replicas\": number_of_replicas\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"},\n",
    "            \"document_id\": {\"type\": \"keyword\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "    # Recreate the index by deleting if it exists and then creating with new settings\n",
    "    if es_client.indices.exists(index=index_name):\n",
    "        es_client.indices.delete(index=index_name)\n",
    "        print(f'Index {index_name} deleted')\n",
    "\n",
    "    es_client.indices.create(index=index_name, body=index_settings)\n",
    "    print('Index created with properties:')\n",
    "    print(json.dumps(index_settings, indent=2))\n",
    "    \n",
    "\n",
    "    count = len(documents)\n",
    "    print(f'Indexing {count} documents to Elasticsearch index {index_name}')\n",
    "    last_document = None\n",
    "    for idx, document in enumerate(documents):\n",
    "        if idx % 100 == 0:\n",
    "\t\t        print(f'{idx + 1}/{count}')\n",
    "\n",
    "        es_client.index(index=index_name, document=document)\n",
    "        last_document = document\n",
    "    if last_document:\n",
    "        print('Last document indexed:')\n",
    "        print(json.dumps(last_document, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Export\n",
    "#### What's the last document id and the index name.\n",
    "#### Anwer: document_id: d8c4c7bb | index_name: documents_20240819_2747\n",
    "```\n",
    "index name: documents_20240819_2747\n",
    "Connecting to Elasticsearch at http://elasticsearch:9200\n",
    "index name: documents_20240819_2747\n",
    "Connecting to Elasticsearch at http://elasticsearch:9200\n",
    "Index created with properties:\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"number_of_shards\": 1,\n",
    "    \"number_of_replicas\": \"0\"\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"text\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"section\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"question\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"course\": {\n",
    "        \"type\": \"keyword\"\n",
    "      },\n",
    "      \"document_id\": {\n",
    "        \"type\": \"keyword\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "Indexing 86 documents to Elasticsearch index documents_20240819_2747\n",
    "1/86\n",
    "\n",
    "\n",
    "\n",
    "Last document indexed:\n",
    "{\n",
    "  \"text\": \"Answer\",\n",
    "  \"section\": \"Workshops: X\",\n",
    "  \"question\": \"Question\",\n",
    "  \"course\": \"llm-zoomcamp\",\n",
    "  \"document_id\": \"d8c4c7bb\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the retrieval\n",
    "#### 1. Select inference --> retrieval --> iterative retrieval\n",
    "#### 2. In iterative retrieval select Add block --> custom code\n",
    "#### Change the index name in iterative retrieval(documents_20240819_2747).\n",
    "#### Paste the following code in edit section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_retrieval_elasticsearch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_retrieval_elasticsearch.py\n",
    "#iterative retrieval\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch, exceptions\n",
    "\n",
    "\n",
    "SAMPLE_QUERY = \"When is the next cohort?\"\n",
    "\n",
    "\n",
    "@data_loader\n",
    "def search(*args, **kwargs) -> List[Dict]:\n",
    "    \n",
    "    \n",
    "    \n",
    "    connection_string = kwargs.get('connection_string', 'http://localhost:9200')\n",
    "    index_name = kwargs.get('index_name', 'documents')\n",
    "    \n",
    "    \n",
    "\n",
    "    script_query = {\n",
    "        \"size\": 1,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\":SAMPLE_QUERY,\n",
    "                        \"fields\": [\"question^5\", \"text\", \"section\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": 'llm-zoomcamp'\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"Sending script query:\", script_query)\n",
    "\n",
    "    es_client = Elasticsearch(connection_string)\n",
    "    \n",
    "    try:\n",
    "        response = es_client.search(\n",
    "            index=index_name,\n",
    "            body= script_query,\n",
    "                \n",
    "            \n",
    "        )\n",
    "\n",
    "        print(\"Raw response from Elasticsearch:\", response)\n",
    "\n",
    "        return [hit['_source'] for hit in response['hits']['hits']]\n",
    "    \n",
    "    except exceptions.BadRequestError as e:\n",
    "        print(f\"BadRequestError: {e.info}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Testing the retrieval\n",
    "#### Use the query: \"When is the next cohort?\"\n",
    "#### What's the ID of the top matching result?\n",
    "#### Answer: bf024675\n",
    "\n",
    "```\n",
    "\n",
    "Sending script query: {'size': 1, 'query': {'bool': {'must': {'multi_match': {'query': 'When is the next cohort?', 'fields': ['question^5', 'text', 'section'], 'type': 'best_fields'}}, 'filter': {'term': {'course': 'llm-zoomcamp'}}}}}\n",
    "\n",
    "Raw response from Elasticsearch: {'took': 4, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 68, 'relation': 'eq'}, 'max_score': 42.219727, 'hits': [{'_index': 'documents_20240819_2747', '_id': 't9tSaZEBWNW8oyffAbuq', '_score': 42.219727, '_source': {'text': 'Summer 2025 (via Alexey).', 'section': 'General course-related questions', 'question': 'When will the course be offered next?', 'course': 'llm-zoomcamp', 'document_id': 'bf024675'}}]}}\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reindexing\n",
    "#### Follow the above steps from ingestion\n",
    "#### Elasticsearch index for FAQ document: version 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#new doc index\n",
    "index name: documents_20240819_2652\n",
    "Connecting to Elasticsearch at http://elasticsearch:9200\n",
    "index name: documents_20240819_2652\n",
    "Connecting to Elasticsearch at http://elasticsearch:9200\n",
    "Index created with properties:\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"number_of_shards\": 1,\n",
    "    \"number_of_replicas\": \"0\"\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"text\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"section\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"question\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"course\": {\n",
    "        \"type\": \"keyword\"\n",
    "      },\n",
    "      \"document_id\": {\n",
    "        \"type\": \"keyword\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "Indexing 86 documents to Elasticsearch index documents_20240819_2652\n",
    "\n",
    "\n",
    "Last document indexed:\n",
    "{\n",
    "  \"text\": \"Answer\",\n",
    "  \"section\": \"Workshops: X\",\n",
    "  \"question\": \"Question\",\n",
    "  \"course\": \"llm-zoomcamp\",\n",
    "  \"document_id\": \"d8c4c7bb\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Reindexing\n",
    "#### Change the index name in iterative retrieval(documents_20240819_2652).\n",
    "#### Use the query \"When is the next cohort?\". What's the ID of the top matching result?\n",
    "#### Answer: b6fa77f3\n",
    "\n",
    "```\n",
    "Sending script query: {'size': 1, 'query': {'bool': {'must': {'multi_match': {'query': 'When is the next cohort?', 'fields': ['question^5', 'text', 'section'], 'type': 'best_fields'}}}}}\n",
    "\n",
    "\n",
    "\n",
    "Raw response from Elasticsearch: {'took': 39, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 68, 'relation': 'eq'}, 'max_score': 86.06232, 'hits': [{'_index': 'documents_20240819_2652', '_id': 'a9S_aZEB0Xc6WBWVCDAH', '_score': 86.06232, '_source': {'text': 'Summer 2026.', 'section': 'General course-related questions', 'question': 'When is the next cohort?', 'course': 'llm-zoomcamp', 'document_id': 'b6fa77f3'}}]}}\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
